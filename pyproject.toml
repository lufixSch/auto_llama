[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "auto-llama"
version = "0.0.2"
description = "Supercharge your local LLM with different agents"
readme = "README.md"
license = { file = "LICENSE" }
keywords = ["AI", "LLM", "Llama"]
authors = [
  { name = "LufixSch" }
]
classifiers = [
   "Programming Language :: Python :: 3",
   "Development Status :: 3 - Alpha",
   "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
   "Operating System :: OS Independent",
]
dependencies = []

[project.optional-dependencies]
"module.nlp" = [
  "num2words",
  "nltk",
  "spacy",
  "coreferee",
  "txtai"
]
"module.react" = []
module = [
  "auto-llama[module.nlp]",
  "auto-llama[module.react]"
]
"agent.research" = [
  "wikipedia",
  "duckduckgo_search",
  "auto-llama[module.nlp]"
]
"agent.code" = [
  "docker",
  "pandas",
  "requests"
]
agent = [
  'auto-llama[agent.research,  agent.code]'
]
"llm.openai" = [
  "openai"
]
llm = [
  'auto-llama[llm.openai]'
]
"memory.txtai" = [
  "auto-llama[module.nlp]"
]
memory = [
  "auto-llama[memory.txtai]"
]
"manager.txtai" = [
  "auto-llama[module.nlp]"
]
manager = [
  "auto_llama[manager.txtai]"
]
"speech.txtai" = [
  "txtai[pipeline-audio]"
]
speech = [
  "auto_llama[speech.txtai]"
]
all = [
  "auto-llama[module, agent, llm, memory, speech]"
]

[tool.hatch.build]
packages = [
  "pkg/auto_llama/",
]

[tool.black]
line-length = 120
include = '\.pyi?$'

[tool.isort]
profile = 'black'
extend_skip = ['__init__.py']
