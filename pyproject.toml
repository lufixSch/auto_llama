[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "auto-llama"
version = "0.0.3"
description = "Supercharge your local LLM with different agents"
readme = "README.md"
license = { file = "LICENSE" }
keywords = ["AI", "LLM", "Llama"]
authors = [
  { name = "LufixSch" }
]
classifiers = [
   "Programming Language :: Python :: 3",
   "Development Status :: 3 - Alpha",
   "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
   "Operating System :: OS Independent",
]
dependencies = [
  "Pillow",
  "numpy"
]

[project.optional-dependencies]
"extras.text" = [
  "num2words",
  "nltk",
  "spacy",
  "coreferee",
  "txtai",
  "beautifulsoup4",
  "pypdf"
]
"extras.audio" = [
  "txtai[pipeline-audio]"
]
extras = [
  "auto-llama[extras.text]",
  "auto-llama[extras.audio]"
]
"agents.research" = [
  "wikipedia",
  "duckduckgo_search",
  "arxiv",
  "auto-llama[text]"
]
"agents.code" = [
  "docker",
  "pandas",
  "requests"
]
"agents.similarity" = [
  'txtai'
]
agents = [
  'auto-llama[agents.research,  agents.code, agebts.similarity]',
]
"llm.openai" = [
  "openai"
]
llm = [
  'auto-llama[llm.openai]'
]
"memory.txtai" = [
  "auto-llama[module.nlp]"
]
memory = [
  "auto-llama[memory.txtai]"
]
"selector.txtai" = [
  "auto-llama[extras.text]"
]
selector = [
  "auto-llama[selector.txtai]"
]
"preprocessor.coref" = [
  "auto-llama[extras.text]"
]
preprocessor = [
  "auto-llama[preprocessor.coref]"
]
all = [
  "auto-llama[extras, agent, llm, memory, selector]"
]

[project.scripts]
auto-llama = "auto_llama_cli:chat_main"
auto-llama-rag = "auto_llama_cli:rag_main"


[tool.hatch.build]
packages = [
  "pkg/auto_llama/",
  "pkg/auto_llama_extras/",
  "pkg/auto_llama_cli/"
]

[tool.black]
line-length = 120
include = '\.pyi?$'

[tool.isort]
profile = 'black'
extend_skip = ['__init__.py']
